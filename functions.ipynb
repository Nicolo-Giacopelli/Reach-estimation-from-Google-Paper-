{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24f1da53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 20:27:26.215067: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-03 20:27:26.215100: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import time\n",
    "import openpyxl\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "import missingno as msno\n",
    "from snn import SyntheticNearestNeighbors\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import xgboost as xgb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow_addons.metrics import RSquare\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import DenseFeatures\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.feature_column import make_parse_example_spec\n",
    "from tensorflow.feature_column import numeric_column\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from mpl_toolkits import mplot3d\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.color_palette(\"pastel\")\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import LinearConstraint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49da31a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(start_time=None, string=None):\n",
    "    '''\n",
    "    Function to compute the time\n",
    "    start_time : starting time generated calling this function without arguments the first time\n",
    "    string: visualization purposes (task description)\n",
    "    '''\n",
    "    if not start_time:\n",
    "        start_time=datetime.datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.datetime.now()-start_time).total_seconds(),3600)\n",
    "        tmin, tsec = divmod(temp_sec,60)\n",
    "        pr = \"for \" + str(string) + \" \" if string else \"\"\n",
    "        print(\"Execution time\", pr, \"is \", thour,\" h :\", tmin,' m :', round(tsec,2), \" s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97595487-e53c-45f2-894f-969363c1cf72",
   "metadata": {},
   "source": [
    "### Generation simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f98fb5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_LCM(P, J, K, MoyLCM, Mass0, N_tableaux, Weight_fixed=False, snn=False):\n",
    "    '''\n",
    "    Function for the creation of the complete matrix (still to be preprocessed)\n",
    "    P : size of population\n",
    "    J: number de devices\n",
    "    K : Number of user types\n",
    "    MoyLCM : Mean of the GMM\n",
    "    N_tableaux : number of sub-campaigns (n. of tableaux to generate)\n",
    "    Weight_fixed : if True, the Dirac weights in 0 are all equal to Mass0\n",
    "                    if False, the weights are chosen in [0, 1] stochastically\n",
    "    '''\n",
    "    # Init\n",
    "    users=[i for i in range(1,P+1)]\n",
    "    Tableau={}\n",
    "    alpha_proportion={}\n",
    "    True_Z={}\n",
    "    alpha={}\n",
    "    theta=np.zeros((K,J))\n",
    "    \n",
    "    # Weights of Dirac to induce sparsity\n",
    "    if Weight_fixed:\n",
    "        poids_0=Mass0*np.ones((K,J))  # used as threshold to fill with 0 (compared to np.random.rand for the decision)\n",
    "    else:\n",
    "        poids_0=Mass0*np.random.rand(K,J)\n",
    "    print(f'{poids_0} are the thresholds for sparsity \\n')\n",
    "    \n",
    "    # Loop over types to create parameters of K*J Poissons\n",
    "    for k in range(K):  \n",
    "        A = np.random.rand(J,J)  # since dimension of Mixture Gaussian is J\n",
    "        B = np.dot(A,A.transpose())   # variance eof Mixture Gaussian (random and definitive positive) diff x type\n",
    "        alpha[k]=np.random.multivariate_normal(np.array([MoyLCM for m in range(J)]),B)  # centre is (moy)*J_dim\n",
    "        for j in range(len(alpha[k])):\n",
    "            theta[k][j]=np.exp(alpha[k][j])  # parameters of poisson \n",
    "    print(f'{theta} are the parameters for the KxJ Poissons \\n')\n",
    "        \n",
    "    # Creation of subcampaigns (full matrices)\n",
    "    for n_t in range(N_tableaux):\n",
    "        d={}\n",
    "        # random assignment of types in [0, K] for each user of each subcampaign\n",
    "        True_Z[n_t]=np.random.randint(0,K,size=P)\n",
    "        alpha_proportion[n_t]=[0 for i in range(K)]\n",
    "        \n",
    "        Z=True_Z[n_t]\n",
    "        # storing of proportion of types\n",
    "        for i in Counter(Z):   # (not) sorted\n",
    "            alpha_proportion[n_t][i]=Counter(Z)[i]/len(Z)    \n",
    "        alpha_proportion[n_t]=np.array(alpha_proportion[n_t])\n",
    "        \n",
    "        users=[n_t*P+i for i in range(1,P+1)]   # list with unique id for P users of Tableau that is being generated\n",
    "        d['users']=users\n",
    "        \n",
    "        for j in range(J):\n",
    "            d['J'+str(j+1)]=[]   # d = {'users': [203, 204, ..], 'J1': [], 'J2': [], ..., 'up to J': []}\n",
    "            \n",
    "        for i in range(P):\n",
    "            k0=True_Z[n_t][i]\n",
    "            for j in range(J):\n",
    "                if np.random.rand()<=poids_0[k0][j]:  # Dirac weights tell probability of sparsity (filled with 0)\n",
    "                    d['J'+str(j+1)]+=[np.nan] if snn==True else [0]                        \n",
    "                else:\n",
    "                    d['J'+str(j+1)]+=[np.random.poisson(theta[k0][j])]  # else used appropriate Poisson parameter (x user type & x device) to fill\n",
    "                    \n",
    "        # storing\n",
    "        df_sans_independance=pd.DataFrame.from_dict(d)\n",
    "        Tableau[n_t]=df_sans_independance[['users']+['J' +str(j+1) for j in range(J)]]\n",
    "      \n",
    "    return Tableau, True_Z, theta, alpha_proportion, poids_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d7fd634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reach_fun_RI(Nuj): #Fonction pour calculer le reach sur un tableau numpy\n",
    "    '''\n",
    "    Function to compute the reach function (in percentage over P) given the matrix of a subcampaign\n",
    "    '''\n",
    "    P=len(Nuj)\n",
    "    reach_true=0\n",
    "    for u in range(P):\n",
    "        if np.sum(Nuj.values[u][1:])>0:\n",
    "            reach_true+=1.0/P\n",
    "    return reach_true\n",
    "\n",
    "def preprocess_dataset(Tableau, prop_panel=1):\n",
    "    '''\n",
    "    (Timed) function to compute the reach function and the normalized vector of sum of cookies\n",
    "    '''\n",
    "    P = Tableau[0].shape[0]\n",
    "    J = Tableau[0].shape[1]-1\n",
    "    \n",
    "    X_panel, Y_panel = [], []\n",
    "    \n",
    "    assert prop_panel<=1,\"The proportion of panelists must be lower (or equal) than 1\"\n",
    "    result = {}\n",
    "    \n",
    "    # start timer\n",
    "    start = timer()\n",
    "    \n",
    "    # computations reach and vector of counts for each sub-campaign\n",
    "    for n in range(len(Tableau)):\n",
    "        T_aux=Tableau[n].to_numpy()\n",
    "        reach_panelist=reach_fun_RI(Tableau[n][Tableau[n]['users']<=(n*P)+P*prop_panel])\n",
    "\n",
    "        c_panelist=list(Tableau[n][Tableau[n]['users']<=(n*P)+P*prop_panel].sum()/(P*prop_panel) )[1:]  # /P for normalization of vector count\n",
    "        c_non_panelist=[(Tableau[n].sum()/(P*(1-prop_panel)))[i+1]-c_panelist[i] for i in range(J)] if prop_panel!=1 else [0 for i in range(J)]\n",
    "        \n",
    "        # result stores everything, separately we build regressors (normalized vector of cookies count) and dependent (calculation of reach function)\n",
    "        result[n]=(T_aux, c_panelist, reach_panelist, c_non_panelist)\n",
    "        X_panel.append(np.array(c_panelist))  \n",
    "        Y_panel.append(np.array(reach_panelist))\n",
    "        \n",
    "    # regressors\n",
    "    X = np.reshape(X_panel,(len(Tableau),J))  # transform in arrays\n",
    "    \n",
    "    # target\n",
    "    Y = np.reshape(Y_panel, (len(Tableau), 1))\n",
    "    \n",
    "    # visualization of timer with description of task\n",
    "    timer(start, \"preprocessing the dataset\")\n",
    "    \n",
    "    return result, X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc1a3f4-b191-4280-9c35-d82ce3d1c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(X_train, X_test, Y_train, Y_test):\n",
    "    ''' Function used to create engineered features as outputs of a deep neural network'''\n",
    "    \n",
    "    clear_session()\n",
    "    \n",
    "    model=Sequential()  #name\n",
    "    model.add(BatchNormalization(input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(30, name='first_hidden', kernel_constraint=MaxNorm(3.0)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(100, name='second_hidden', kernel_constraint=MaxNorm(3.0)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(30, name='third_hidden', kernel_constraint=MaxNorm(3.0)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='linear', name='output'))\n",
    "    model.compile(optimizer=RMSprop(momentum=0.9), loss=MeanSquaredError(), metrics=[RSquare(), RootMeanSquaredError()])  # loss_weights = ..., \n",
    "    model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=0, batch_size=32)\n",
    "    model.summary()\n",
    "    \n",
    "    wanted_layer = 'second_hidden'\n",
    "    feature_model = Model(inputs=model.input, outputs=model.get_layer(wanted_layer).output)\n",
    "    \n",
    "    addit_feature_train = feature_model.predict(X_train)  # dropout not used when predictions, it knows in test mode automatically\n",
    "    addit_feature_test = feature_model.predict(X_test)\n",
    "    X_train_eng = np.concatenate((X_train, addit_feature_train), axis=1)\n",
    "    X_test_eng = np.concatenate((X_test, addit_feature_test), axis=1)\n",
    "    print(f\"Engineered X_train with number of features = {X_train_eng.shape[1]}\")\n",
    "    print(f\"Engineered X_test with number of features = {X_test_eng.shape[1]}\")\n",
    "    return X_train_eng, X_test_eng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059a7413-1415-4629-a7e5-87f516229b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_1hidden_model(batch_norm=False, dropout_rate_init=0, dropout_rate_hidd=0, init_mode='uniform', activ_hidd='relu',\n",
    "                         activ_out='linear', constraint=5.0, neurons=10, loss_=MeanSquaredError()):\n",
    "    '''\n",
    "    feature_outputs is output of above function (dense Tensors)\n",
    "    '''\n",
    "    clear_session()\n",
    "    \n",
    "    model=Sequential()  #name\n",
    "    if batch_norm==True:\n",
    "        model.add(BatchNormalization(input_shape=(input_dim,)))\n",
    "        model.add(Dropout(dropout_rate_init))\n",
    "    else: \n",
    "        model.add(Dropout(dropout_rate_init, input_shape=(input_dim,)))\n",
    "    model.add(Dense(neurons, activation=activ_hidd, kernel_initializer=init_mode, name='hidden_layer', kernel_constraint=MaxNorm(constraint)))\n",
    "    if batch_norm==True:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate_hidd))\n",
    "    model.add(Dense(1, activation=activ_out, kernel_initializer=init_mode, name='output'))\n",
    "    model.compile(optimizer=RMSprop(momentum=0.9), loss=loss_, metrics=[RSquare(), RootMeanSquaredError()])  # loss_weights = ..., \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a731d4-ea2f-45a6-9832-137ae06b17d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_2hidden_model(batch_norm=False, dropout_rate_init=0, dropout_rate_hidd=0, init_mode='uniform', activ_hidd='relu',\n",
    "                         activ_out='linear', constraint=5.0, neurons_1=10, neurons_2=5, loss_=MeanSquaredError()):\n",
    "    '''\n",
    "    feature_outputs is output of above function (dense Tensors)\n",
    "    '''\n",
    "    clear_session()\n",
    "    \n",
    "    model=Sequential()  #name\n",
    "    if batch_norm==True:\n",
    "        model.add(BatchNormalization(input_shape=(input_dim,)))\n",
    "        model.add(Dropout(dropout_rate_init))\n",
    "    else: \n",
    "        model.add(Dropout(dropout_rate_init, input_shape=(input_dim,)))\n",
    "    model.add(Dense(neurons_1, activation=activ_hidd, kernel_initializer=init_mode, name='hidden_layer_1', kernel_constraint=MaxNorm(constraint)))\n",
    "    if batch_norm==True:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate_hidd))\n",
    "    model.add(Dense(neurons_2, activation=activ_hidd, kernel_initializer=init_mode, name='hidden_layer_2', kernel_constraint=MaxNorm(constraint)))\n",
    "    if batch_norm==True:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate_hidd))\n",
    "    model.add(Dense(1, activation=activ_out, kernel_initializer=init_mode, name='output'))\n",
    "    model.compile(loss=loss_, metrics=[RSquare(), RootMeanSquaredError()])  # loss_weights = ..., \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c06b0d1-8e5a-49e9-a35d-b23e1c3b3f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_3hidden_model(batch_norm=False, dropout_rate_init=0, dropout_rate_hidd=0, init_mode='uniform', activ_hidd='relu',\n",
    "                         activ_out='linear', constraint=5.0, neurons_1=10, neurons_2=5, neurons_3=2, loss_=MeanSquaredError()):\n",
    "    '''\n",
    "    feature_outputs is output of above function (dense Tensors)\n",
    "    '''\n",
    "    clear_session()\n",
    "    \n",
    "    model=Sequential()  #name\n",
    "    if batch_norm==True:\n",
    "        model.add(BatchNormalization(input_shape=(input_dim,)))\n",
    "        model.add(Dropout(dropout_rate_init))\n",
    "    else: \n",
    "        model.add(Dropout(dropout_rate_init, input_shape=(input_dim,)))\n",
    "    model.add(Dense(neurons_1, activation=activ_hidd, kernel_initializer=init_mode, name='hidden_layer_1', kernel_constraint=MaxNorm(constraint)))\n",
    "    if batch_norm==True:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate_hidd))\n",
    "    model.add(Dense(neurons_2, activation=activ_hidd, kernel_initializer=init_mode, name='hidden_layer_2', kernel_constraint=MaxNorm(constraint)))\n",
    "    if batch_norm==True:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate_hidd))\n",
    "    model.add(Dense(neurons_3, activation=activ_hidd, kernel_initializer=init_mode, name='hidden_layer_3', kernel_constraint=MaxNorm(constraint)))\n",
    "    if batch_norm==True:\n",
    "        model.add(BatchNormalization())\n",
    "    # without final Dropout\n",
    "    model.add(Dense(1, activation=activ_out, kernel_initializer=init_mode, name='output'))\n",
    "    model.compile(loss=loss_, metrics=[RSquare(), RootMeanSquaredError()])  # loss_weights = ..., \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea0f64a-015c-4fbb-9ac5-a1741d00bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nets(history):\n",
    "    rmse_lst = history.history['root_mean_squared_error']\n",
    "    val_rmse_lst = history.history['val_root_mean_squared_error']\n",
    "    \n",
    "    r2_lst = history.history['r_square']\n",
    "    val_r2_lst = history.history['val_r_square']\n",
    "                  \n",
    "    fig, ax = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    x_axis=range(1, len(rmse_lst)+1)  # epochs\n",
    "    ax[0].plot(x_axis, rmse_lst, 'r', label=\"Train dataset\")\n",
    "    ax[0].plot(x_axis, val_rmse_lst, 'b', label=\"Evaluation dataset\")\n",
    "    ax[0].set_title(\"Evolution of RMSE in Training and Test dataset\")\n",
    "    ax[1].plot(x_axis, r2_lst, 'r', label=\"Train dataset\")\n",
    "    ax[1].plot(x_axis, val_r2_lst, 'b', label=\"Evaluation dataset\")\n",
    "    ax[1].set_title(\"Evolution of R2 in Training and Test dataset\")\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae01964-12b9-45fb-8401-11e741537514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_decay(epoch):\n",
    "    lrate= lr * np.exp(-decay*epoch)\n",
    "    return lrate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeace69b-9d3f-4cdf-8ede-bf5d3588a479",
   "metadata": {},
   "source": [
    "### Generation real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41a753f2-71dc-4d69-ac55-cb7da3078c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_data(resp_level, d_group=None):\n",
    "    '''\n",
    "    resp_level:  Mediametrie data as provided (csv file)\n",
    "    d_group: if None computed for all dem_group, otherwise list with demographic ID to generate data for\n",
    "    \n",
    "    output: dict with keys = demographic group ID and \n",
    "                 and values = dict with keys = n. of campaigns and \n",
    "                                        values = dict as \n",
    "                            {'full': entire N_uj matrix, 'reach': int, n. of unique user reached (normalized by P), \n",
    "                            'sum': result_campaign.sum(axis=0)/result_campaign.shape[0] }   \n",
    "    '''\n",
    "    \n",
    "    # start timer\n",
    "    start = timer()\n",
    "    \n",
    "    dem_group = sorted(resp_level[\"Demographic Group\"].unique().tolist()) if d_group==None else sorted(d_group)\n",
    "    fig, ax = plt.subplots(math.ceil(len(dem_group)/2), 2, figsize=(14, 7*math.ceil(len(dem_group)/2)))\n",
    "    \n",
    "    campaigns = sorted(resp_level[\"Campaign ID\"].unique().tolist())\n",
    "    cookies_type = resp_level[\"Station\"].unique().tolist()\n",
    "    cookies_dict = {cookies_type[i]: list(range(len(cookies_type)))[i] for i in range(len(cookies_type))}  # dict for Station (cookies types)\n",
    "\n",
    "    final_store = {}\n",
    "\n",
    "    for group in dem_group:\n",
    "        print(f\"\\nDemographic group: {group}\")\n",
    "        number_rows_tot, reach_print = [], []\n",
    "        campaign_store = {}\n",
    "        to_plot = {list(range(1, len(cookies_type)+1))[i]: [0, 0.0] for i in range(len(cookies_type))}\n",
    "        # n. of cookies used by campaign (at least one observation): [count campaigns, sum of reach]\n",
    "        \n",
    "        dem_slice = resp_level[resp_level[\"Demographic Group\"]==group]\n",
    "        for campaign in campaigns:\n",
    "            camp_slice = dem_slice[dem_slice[\"Campaign ID\"]==campaign]\n",
    "            assert camp_slice.shape[0]>0, \"better check\"  # all campaign should be in all demographic group\n",
    "\n",
    "            serial_lst = sorted(list(camp_slice[\"Serial\"].unique()))\n",
    "            weight_lst = camp_slice.sort_values(by=\"Serial\", axis=0, inplace=False) \\\n",
    "                            .groupby(\"Serial\")[\"Weight\"].unique().reset_index()[\"Weight\"].astype(\"int\").values\n",
    "            weight_cum = np.cumsum(weight_lst)\n",
    "            weight_dict = {serial_lst[i]: weight_lst[i] for i in range(len(serial_lst))} # to find subset of lines for specific Serial\n",
    "            weight_dict_cum = {serial_lst[i]: weight_cum[i] for i in range(len(serial_lst))} # to find subset of lines for specific Serial\n",
    "\n",
    "            result_campaign = np.zeros((weight_cum[-1], len(cookies_type)))  # total matrix \n",
    "            # rows = (weights are only those inside campaign, depending on Serial) & columns = (made of all cookied types, even not used)\n",
    "            number_rows_tot.append(weight_cum[-1])\n",
    "            \n",
    "            cookies_printed = len(camp_slice[\"Station\"].unique().tolist())\n",
    "            to_plot[cookies_printed][0] += 1\n",
    "\n",
    "            for u, serial in enumerate(serial_lst):\n",
    "                serial_slice = camp_slice[camp_slice[\"Serial\"]==serial]\n",
    "\n",
    "                top_r = 0 if u==0 else weight_dict_cum[serial_lst[u-1]]\n",
    "                bottom_r = weight_dict_cum[serial]\n",
    "                slice_rows = np.arange(top_r, bottom_r, dtype=int)\n",
    "\n",
    "                cookies_used = serial_slice[\"Station\"].unique().tolist()\n",
    "                for stat in cookies_used:\n",
    "                    station_slice = serial_slice[serial_slice[\"Station\"]==stat]\n",
    "                    \n",
    "                    prop_viewed = station_slice[\"Proportion of Ad Break viewed\"].values\n",
    "                    tot_avg, tot_left = 0, 0\n",
    "                    for viewed in prop_viewed:  # Ads Break are not even filtered by\n",
    "                        num = viewed*weight_dict[serial]\n",
    "                        avg, addit = divmod(num, weight_dict[serial])                    \n",
    "                        tot_avg += avg\n",
    "                        tot_left += addit\n",
    "\n",
    "                    final_n, final_left = divmod(np.rint(tot_left), weight_dict[serial])\n",
    "                    tot_avg += final_n\n",
    "\n",
    "                    random_rows = np.random.choice(slice_rows, size=np.rint(final_left).astype(int), replace=False)\n",
    "                    result_campaign[slice_rows, cookies_dict[stat]] += tot_avg\n",
    "                    result_campaign[random_rows, cookies_dict[stat]] += 1 \n",
    "            \n",
    "            reach = 0.0\n",
    "            for row in range(result_campaign.shape[0]):\n",
    "                if result_campaign[row].sum() > 0:\n",
    "                    reach += 1\n",
    "            reach /= result_campaign.shape[0]\n",
    "            reach_print.append(reach)\n",
    "\n",
    "            campaign_store[campaign] = {'full': np.copy(result_campaign), \n",
    "                                        'reach': reach,\n",
    "                                        'sum': result_campaign.sum(axis=0)/result_campaign.shape[0]\n",
    "                                       }            \n",
    "                \n",
    "            to_plot[cookies_printed][1] += reach\n",
    "        \n",
    "        avg_reach_station = []\n",
    "        for n_station_seen, dat in to_plot.items():\n",
    "            avg_reach_station.append(dat[1]/float(dat[0]))   # mean of reach for each unique station seen (fixing demographic group))\n",
    "                                     \n",
    "        len_station_seen = np.array(list(to_plot.keys()))\n",
    "        avg_reach_station = np.array(avg_reach_station)\n",
    "        \n",
    "        axs = [x for row in ax for x in row] # flatten for plot                     \n",
    "        axs[dem_group.index(group)].plot(len_station_seen, avg_reach_station, linewidth=2)\n",
    "        axs[dem_group.index(group)].set_xlabel('N. distinct TV-channels seen within a campaign', fontsize=15)\n",
    "        axs[dem_group.index(group)].set_ylabel(r'Average estimated reach across campaign', fontsize=15)\n",
    "        axs[dem_group.index(group)].set_title(f\"Dem group {group}\", fontsize=20)      \n",
    "        \n",
    "        final_store[group] = campaign_store\n",
    "        \n",
    "        number_rows_avg = np.array(number_rows_tot).mean()\n",
    "        reach_print_avg = np.array(reach_print).mean()\n",
    "        print(f\"Average number of users observed (P): {int(number_rows_avg)}\")\n",
    "        print(f\"Average reach/P observed (Reach): {reach_print_avg: .5f}\\n\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "     \n",
    "    timer(start, \"preprocessing Mediametrie data\")\n",
    "    return final_store, fig           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70991f83-4fa6-4ad7-9d5e-dae183129cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_real_data(final_store, test=False):\n",
    "    '''\n",
    "    function used for ML model data generation from Mediametrie data\n",
    "    final_store: complex dict, result of generate_real_data\n",
    "    test: bool to indicate whether we are processing October (train set) or January (test), \n",
    "          as they have different number of campaigns (there is an assert to be sure)\n",
    "    \n",
    "    returns: dict with keys=demographic ID and values=list of couples of np.arrays X, Y, \n",
    "             where X is sum of normalized sum of cookies count and Y is the estimated reach/P\n",
    "             (for each of the demographic groups in the final_store provided)\n",
    "    \n",
    "    '''\n",
    "    final = {}\n",
    "    for dem_group, df_camp in final_store.items():\n",
    "        X_pan, Y_pan = [], []\n",
    "        for n_camp, data in df_camp.items():\n",
    "            X_pan.append(data['sum'])\n",
    "            Y_pan.append(float(data['reach']))\n",
    "        n_camp = len(df_camp.keys())\n",
    "        if test:\n",
    "            assert n_camp == 418, 'better check'  # each demographic group should exhibit all 544 campaigns\n",
    "        else: \n",
    "            assert n_camp == 544, 'better check'  # each demographic group should exhibit all 544 campaigns\n",
    "        \n",
    "        # regressors\n",
    "        X = np.reshape(X_pan,(n_camp, len(X_pan[-1])))  # transform in arrays\n",
    "        Y = np.reshape(Y_pan,(n_camp, 1)) \n",
    "        \n",
    "        final[dem_group] = [X, Y]\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca95eea3-9bbd-43a5-bd6d-aae3a4f7a91d",
   "metadata": {},
   "source": [
    "### Synthetic Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf835503-7354-474b-8b80-f8be4607ba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_columns(Tab):\n",
    "    '''\n",
    "    function used to insert 0-1 on top and normalize columns by their sum (each user)\n",
    "    Tab: np.array already transposed\n",
    "    '''\n",
    "    top_pad = []\n",
    "    P = Tab.shape[1]\n",
    "    w_us = Tab.astype(\"float\")   # without users\n",
    "    \n",
    "    for col in range(P):\n",
    "        col_sum=np.nansum(w_us[:, col]).astype(\"float\")\n",
    "        if col_sum==0:\n",
    "            top_pad.append(0)\n",
    "        else:\n",
    "            w_us[:, col] = np.divide(w_us[:, col], col_sum)  # each column (user, with cookies=row since .T) is normalized with the sum\n",
    "            top_pad.append(1)\n",
    "    final = np.vstack((np.array(top_pad), w_us))\n",
    "    return final\n",
    "\n",
    "def sample_panelist(True_Z, alpha_proportion, P_panelist):\n",
    "    '''\n",
    "    Function used looping over all campaigns to obtain a number of panelist from a bigger dataset preserving\n",
    "    the a priori proportion of user types\n",
    "    \n",
    "\n",
    "    True_z: is np array of size P (it is one value of a bigger dict output of dataset_LCM, with keys = Campaign id), \n",
    "              made of random int between 0 and (true) K\n",
    "    alpha_proportion: is np array of size K, again as above it is a value of a bigger dict passed to the function\n",
    "    P panelist: are the a priori wanted number of panelists (there are rounding errors \n",
    "                as proportion*Panelist is approximated to an integer)\n",
    "    '''\n",
    "    \n",
    "    real_K = len(alpha_proportion)\n",
    "    \n",
    "    random_cols=np.array([]).astype(int)\n",
    "    \n",
    "    prosp_number = [np.rint(alpha_proportion[i]*P_panelist) for i in range(real_K)]\n",
    "    for i in range(real_K):\n",
    "        idx_type = np.where(True_Z==i)[0]\n",
    "        selected = np.random.choice(idx_type, size=int(prosp_number[i]), replace=False) \n",
    "        random_cols = np.concatenate((random_cols, selected.astype(int)), axis=None)\n",
    "    return random_cols\n",
    "\n",
    "def snn_dataset_sim(Tableau, True_Z, alpha_proportion, normalize_sum_reach=False, P_panelist=100):\n",
    "    '''\n",
    "    (Timed) function that takes as input the simulated dataset (dict with keys=campaign ID)\n",
    "    used to add 1st row filled with {0, 1} and last column with sum of collect cookies (for each j in J since transposed)\n",
    "    \n",
    "    the first value in the last column (position 0) is therefore the reach computed as int, which is stored and \n",
    "    substituted with a NaN value to be predicted by the SNN algorithm\n",
    "    \n",
    "    normalize_sum_reach: bool, whether to normalized last column (c mapped to t, and reach to reach/P)\n",
    "    '''\n",
    "    \n",
    "    P = Tableau[0].shape[0]\n",
    "    P_panelist = P if P<P_panelist else P_panelist\n",
    "    J = Tableau[0].shape[1]-1  # there is 'user' column\n",
    "    \n",
    "    # start timer\n",
    "    start = timer()\n",
    "    result = {}\n",
    "    \n",
    "    # computations reach and vector of counts for each sub-campaign\n",
    "    for n in range(len(Tableau)):\n",
    "        T_aux = Tableau[n].to_numpy().T  # transposed\n",
    "        T_aux = discretize_columns(T_aux[1:, :])  # discretization (first row)\n",
    "        \n",
    "        vector_sum=np.nansum(T_aux, axis=1).reshape((-1, 1))\n",
    "        if normalize_sum_reach:\n",
    "            vector_sum = np.divide(vector_sum, P)   # normalized t_i vector and reach \n",
    "        \n",
    "        reach_comp = float(vector_sum[0, 0])\n",
    "        vector_sum[0, 0] = np.nan   \n",
    "        \n",
    "        #slice_columns = np.arange(P, dtype=int)   # not taken care of rappresentation of group K\n",
    "        #random_cols = np.random.choice(slice_columns, size=P_panelist, replace=False)\n",
    "        random_cols = sample_panelist(True_Z[n], alpha_proportion[n], P_panelist)\n",
    "        T_panelist = T_aux[:, random_cols]\n",
    "           \n",
    "        final = np.hstack((T_panelist, vector_sum))\n",
    "        \n",
    "        # result stores everything, separately we build regressors (normalized vector of cookies count) and dependent (calculation of reach function)\n",
    "        result[n]=(final, reach_comp)\n",
    "    \n",
    "    # visualization of timer with description of task\n",
    "    timer(start, \"preprocessing the dataset for SNN\")    \n",
    "    return result\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0e43f6f-e0ec-4157-8ad7-f498e00584f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snn_dataset_real(Tableau, norm_sum_reach=False, P_panelist=100):\n",
    "    '''\n",
    "    function that calls the previous function to handle Mediametrie data = dict with keys: Demographic group ID\n",
    "    (there is the extra layer of demographic group, in addition to the campaign one)\n",
    "    \n",
    "    norm_sum_reach: calls parameter of above function\n",
    "    P = n. of rows to select (number of panelist)\n",
    "    '''\n",
    "    final = {}    \n",
    "    for dem_group, df_camp in Tableau.items():\n",
    "        camp_store = {}\n",
    "        reach_dict = {i: df_camp[i]['reach'] for i in df_camp.keys()}\n",
    "        vector_sum_dict = {i: np.concatenate((np.array(np.nan), df_camp[i]['sum']), axis=None) for i in df_camp.keys()}\n",
    "        users_dict  = {i: df_camp[i]['full'].shape[0] for i in df_camp.keys()}\n",
    "        \n",
    "        for n_camp, camp_mat in df_camp.items():\n",
    "            # !!! here remains random, need to implement True_Z for real data\n",
    "            slice_rows = np.arange(users_dict[n_camp], dtype=int)   # not taken care of rappresentation of group K\n",
    "            random_rows = np.random.choice(slice_rows, size=P_panelist, replace=False)\n",
    "            raw_mat = camp_mat[random_rows, :].T   # transposed\n",
    "            raw_two = discretize_columns(Tab)\n",
    "            \n",
    "            final = np.hstack((raw_two, vector_sum_dict[n_camp]))\n",
    "            camp_store[n_camp] = (final, reach_dict[n_camp])\n",
    "            \n",
    "        final[dem_group] = camp_store   # engmax is dict with keys=campaign ID and values (full matrix, normalized reach) tuple\n",
    "    return final    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f43eeb52-2bf5-4301-bd2b-efd92e3d2cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snn(Tab, model, P_initial, n_comp=None):\n",
    "    '''\n",
    "    function implementing Synthethic Nearest Neighbour algorithm for our specific setup\n",
    "    Tab: dict result of preprocessing (with keys=campaigns)\n",
    "    model: sklearn estimator, with fit and predict methods\n",
    "    P_initial is number of panelist + number of non panelist (Tab is made of panelist + vector count for both)\n",
    "    n_components: if None all components are tried and the evolution of mean squared error is plotted\n",
    "                  if fixed to a number (say n), considered only first n components\n",
    "    '''\n",
    "    r2, rmse = [], []\n",
    "    \n",
    "    P = Tab[0][0].shape[1]-1\n",
    "    scaling_factor = P_initial/P\n",
    "    \n",
    "    n_campaigns = len(Tab)\n",
    "    max_comp = Tab[0][0].shape[0]-1\n",
    "    \n",
    "    if ((n_comp != None) and (n_comp>max_comp)) or (n_comp==0):\n",
    "        print(f\"Number of components must be between 1 and {max_comp}\")\n",
    "        return\n",
    "    \n",
    "    preds, real = [], []\n",
    "    var_explained = []\n",
    "    \n",
    "    for camp, df in Tab.items():\n",
    "        data = pd.DataFrame(df[0].T)\n",
    "        \n",
    "        y = data.iloc[:, 0].values\n",
    "        y_train = y[:-1].reshape(-1, 1)\n",
    "        \n",
    "        real.append(df[1])\n",
    "        \n",
    "        x = data.iloc[:, 1:].values\n",
    "        x_train = x[:-1, :]\n",
    "        x_test = x[-1, :].reshape(1, -1)\n",
    "        \n",
    "        sc = StandardScaler()\n",
    "        pca = PCA()\n",
    "        x_train_pca = pca.fit_transform(sc.fit_transform(x_train))\n",
    "        \n",
    "        mean_sum = P*scaling_factor*sc.mean_\n",
    "        std_sum = np.sqrt(P*scaling_factor)*sc.scale_\n",
    "        x_test = (x_test-mean_sum)/std_sum\n",
    "        x_test_pca = pca.transform(x_test)  # test how x_test is transformed\n",
    "        \n",
    "        if n_comp!=None:\n",
    "            trained = model.fit(x_train_pca[:, :n_comp].reshape(P, n_comp), y_train)\n",
    "            pred = float(trained.predict(x_test_pca[:, :n_comp].reshape(1, n_comp))[0])\n",
    "            preds.append(pred)\n",
    "            var_explained.append(float(np.cumsum(pca.explained_variance_ratio_)[n_comp-1]))\n",
    "        else:\n",
    "            for comp in range(max_comp):\n",
    "                trained = model.fit(x_train_pca[:, :comp+1].reshape(P, comp+1), y_train)\n",
    "                pred = float(trained.predict(x_test_pca[:, :comp+1].reshape(1, comp+1))[0])\n",
    "                if comp==0:\n",
    "                    preds.append([pred])\n",
    "                    var_explained.append([float(np.cumsum(pca.explained_variance_ratio_)[comp])])\n",
    "                else:\n",
    "                    preds[-1].append(pred) \n",
    "                    var_explained[-1].append(float(np.cumsum(pca.explained_variance_ratio_)[comp]))\n",
    "                \n",
    "    real_values = np.array(real)\n",
    "    pred_values = np.array(preds).reshape(n_campaigns, ) if n_comp!=None else np.reshape(preds, (n_campaigns, max_comp))\n",
    "    var_explained = np.array(var_explained) if n_comp!=None else np.reshape(var_explained, (n_campaigns, max_comp))\n",
    "    \n",
    "    if n_comp!=None:\n",
    "        r2 = r2_score(real_values, pred_values)\n",
    "        rmse = np.sqrt(mean_squared_error(real_values, pred_values))\n",
    "        print(f\"\\n SNN based on PCR with {int(n_comp)} component(s)\\n\")\n",
    "        print(f\"Percentage of variance explained: {var_explained.mean(): .2%}\")\n",
    "        print(f\"R2 score across {n_campaigns} campaigns: {r2: .5%}\")\n",
    "        print(f\"Average RMSE score across {n_campaigns} campaigns: {rmse}\")\n",
    "    else:\n",
    "        lst_r2 = [r2_score(real_values, pred_values[:, i]) for i in range(max_comp)]\n",
    "        lst_rmse = [np.sqrt(mean_squared_error(real_values, pred_values[:, i])) for i in range(max_comp)]\n",
    "        var_expl = var_explained.mean(axis=0)\n",
    "        assert np.argmax(lst_r2)==np.argmin(lst_rmse), \"check\"\n",
    "        \n",
    "        print(f\"\\n SNN with analysis on number of components, {n_campaigns} campaigns \\n\")\n",
    "        print(f\" Highest R2 and RMSE score are obtained with {np.argmax(lst_r2)+1} component(s)\")\n",
    "        for i in range(1, max_comp+1):\n",
    "            print(f\"\\n Principal component(s) selected: {int(i)} \\n R2 score: {lst_r2[i-1]: .5%}\")\n",
    "            print(f\" RMSE score: {lst_rmse[i-1]} \\n Percentage of variance explained: {var_expl[i-1]: .2%}\")\n",
    "            \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(14, 5))\n",
    "        plt.plot(list(range(1, max_comp+1)), lst_r2)\n",
    "        plt.xlabel('Number of Principal Components')\n",
    "        plt.ylabel(r'$R^2$ score')\n",
    "        plt.title(r\"Evolution of $R^2$ with n. of PC selected\")       \n",
    "        \n",
    "        \n",
    "# x_test_stand = np.divide(x_test-(P*sc.mean_), np.sqrt(P)*sc.scale_) \n",
    "# x_test (vector of sum of cookies) is not normalized before PCA since would become 0, it is assumded to be normalized by P (or not=\n",
    "# pca_lst = [PCA(n_components=n_comp)] if n_comp!=None else [PCA(n_components=int(i)) for i in range(1, max_comp+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e026da-8644-4ef4-9d31-5870c0fd42e0",
   "metadata": {},
   "source": [
    "### Google's algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67ba5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADM(X_train, Y_train, N, L, sigma, threshold, prop_panel=1, lasso=0, _print_=True, compute_r2=False):   # lasso is a lambda parameter (float)\n",
    "    '''\n",
    "    (Timed) implementation of Google's Adaptive Dirac Mixture algorithm\n",
    "    T: regressors, R: dependent\n",
    "    N are iterations, L additional centers, sigma variance\n",
    "    Threshold is the threshold consider to discard alphas, if None is dynamic with K (works well)\n",
    "    compute_r2 is list of [X_test, Y_test]\n",
    "    '''\n",
    "    \n",
    "    # storing\n",
    "    fun_eval = []\n",
    "    metrics = []\n",
    "    th_list = []\n",
    "    K_list = [1]\n",
    "    J = X_train.shape[1]\n",
    "    \n",
    "    # INIT\n",
    "    K = 1\n",
    "    alpha = [1]\n",
    "    centers_ = [[1 for i in range(J)]]\n",
    "\n",
    "    # N. OF ITERATIONS\n",
    "    for it_ in range(N):         \n",
    "        if _print_:\n",
    "            print(f\"\\n \\t \\t ITER {it_+1}\")\n",
    "        mus = centers_.copy()\n",
    "        # For all additional centers considered\n",
    "        for m in range(L):         \n",
    "            pis = alpha.copy()\n",
    "            acc_pis = [np.sum(pis[:i]) for i in range(1, len(pis)+1)]\n",
    "            # assert np.isclose(acc_pis[-1], 1, atol = 4*1e-2), f\"cum is: {acc_pis[-1]}\" \n",
    "        \n",
    "            # Stochastically select Gaussian from which to generate center (according to their probability)\n",
    "            r = np.random.uniform(0, 1) # sample uniform\n",
    "            k = 0\n",
    "            for i, thresh in enumerate(acc_pis):\n",
    "                if r < thresh:\n",
    "                    k = i\n",
    "                    break\n",
    "            selected_mu = mus[k]  # center considered\n",
    "            selected_cov = sigma**2*np.eye(J)   # variance, same for all centers\n",
    "            # center is generated and appended\n",
    "            samples = np.random.multivariate_normal(selected_mu, selected_cov)  \n",
    "            centers_ += [samples]   # mus is old X\n",
    "        K = K+L\n",
    "        \n",
    "        # take time\n",
    "        start = timer()\n",
    "        \n",
    "        # objective function to minimize to find weights alphas\n",
    "        # it is the Sum of Squared deviation between real value and predictions\n",
    "        # the sum of the absolute values of parameters with the lasso parameter is considered if passed\n",
    "        def objective(alphas): \n",
    "            aux = 0.0\n",
    "            for i in range(X_train.shape[0]):\n",
    "                S_aux=0.0\n",
    "                for k in range(K):\n",
    "                    S_aux += alphas[k]*(1-np.exp(-np.dot(centers_[k], X_train[i])))  # SSE\n",
    "                aux += ((Y_train[i]-S_aux)**2)/float(X_train.shape[0])\n",
    "            aux += lasso*np.sum(np.abs(alphas))  # Lasso penalization\n",
    "            return aux\n",
    "        \n",
    "        def constraint_alpha(alphas):\n",
    "            return -1+np.sum(alphas)  # sum of weights must be equal to 1\n",
    "        \n",
    "        def constraint_alpha_lasso(alphas):\n",
    "            return -1+np.sum(np.abs(alphas))  # sum of abs weights must be equal to 1\n",
    "        \n",
    "        # initial guess for alpha are re-drawn from a Dirichlet distribution every iteration\n",
    "        alpha_initial_guess=stats.dirichlet.rvs([0.5 for i in range(K)])[0].reshape(-1, 1)\n",
    "        \n",
    "        bound = [(0,1) for i in range(K)]  # bounds for the parameters (are probabilities)\n",
    "        bound_lasso = [(-1,1) for i in range(K)]\n",
    "        cons = ({\"type\" : \"eq\", \"fun\" :  constraint_alpha})\n",
    "        cons_lasso = ({\"type\" : \"eq\", \"fun\" :  constraint_alpha_lasso})\n",
    "        \n",
    "        # Minimization\n",
    "        # cons_trust = LinearConstraint(A = np.ones((1,K)), lb=1, ub=1)\n",
    "        # method = 'trust-constr' SLOWER\n",
    "        if lasso!=0:\n",
    "            sol = minimize(objective, alpha_initial_guess.flatten(), method='slsqp', bounds=bound_lasso, constraints=cons_lasso)\n",
    "        else:\n",
    "            sol = minimize(objective, alpha_initial_guess.flatten(), method='slsqp', bounds=bound, constraints=cons)\n",
    "        alpha = sol.x  # Alphas' values minimizing\n",
    "        \n",
    "        loss = float(sol.fun) if lasso==0 else float(sol.fun) - lasso*np.sum(np.abs(alpha))\n",
    "        lasso_pen = 0 if lasso==0 else float(sol.fun - loss)\n",
    "        fun_eval.append(loss)  # Storing evaluation of objective function\n",
    "        \n",
    "        if _print_:\n",
    "            timer(start, 'minimization')  # print timing with description of task\n",
    "            print(f\"\\nLOSS evaluation (without lasso penalization): {fun_eval[-1]}\")\n",
    "            \n",
    "        # THRESHOLD, if not set, use Adaptive (proportion of exp value of probability\n",
    "        sogl = 0.05*(1/K) if threshold==None else threshold  # if thresh None proportion of 1/K, if passed it is taken as such\n",
    "        th_list.append(sogl)\n",
    "        \n",
    "        discard = 0\n",
    "        new_alpha=[]\n",
    "        new_centers=[]\n",
    "        for q in range(len(alpha)):\n",
    "            if alpha[q] < float(sogl): \n",
    "                discard += 1\n",
    "                continue\n",
    "            else:\n",
    "                new_alpha += [alpha[q]]\n",
    "                new_centers += [centers_[q]]\n",
    "        \n",
    "        # update after discarding and store\n",
    "        K = len(new_alpha)\n",
    "        K_list.append(K)\n",
    "        alpha = new_alpha.copy()\n",
    "        centers_ = new_centers.copy()\n",
    "        \n",
    "        if _print_:\n",
    "            print(f\"Considered centers: {len(centers_)}\")\n",
    "            print(f\"Discarded centers: {discard}\")  # print how many discarded centers\n",
    "            \n",
    "        if type(compute_r2)==list:\n",
    "            preds, r2, rmse = eval_ADM(K_list, alpha, centers_, compute_r2[0], compute_r2[1], print_=_print_)\n",
    "            metrics.append([r2, rmse])\n",
    "            \n",
    "        # stopping condition\n",
    "        if it_== 0:\n",
    "            best_alpha = alpha.copy()\n",
    "            best_centers = centers_.copy()\n",
    "            best_iter, disc = 0, 0\n",
    "        else:\n",
    "            if fun_eval[-1] < fun_eval[best_iter]:\n",
    "                best_iter = it_\n",
    "                best_alpha = alpha.copy()\n",
    "                best_centers = centers_.copy()\n",
    "                disc = 0\n",
    "            else:\n",
    "                disc += 1\n",
    "                \n",
    "        if (it_ > 34) and (disc > 9):\n",
    "            alpha = best_alpha.copy()\n",
    "            centers_ = best_centers.copy()\n",
    "            K_list = K_list[:best_iter+2]  # since K_List has one value more (appended at the beginning)\n",
    "            fun_eval = fun_eval[:best_iter+1]\n",
    "            th_list = th_list[:best_iter+1]\n",
    "            if type(compute_r2)==list:\n",
    "                metrics = metrics[:best_iter+1]\n",
    "            if _print_:\n",
    "                print(f'\\n\\nSTOPPING condition at iter {it_+1}, considering best iter {best_iter+1}\\n\\n')\n",
    "            break\n",
    "\n",
    "    result = [alpha, centers_, K_list, fun_eval, th_list]\n",
    "    if type(compute_r2)==list:\n",
    "        result.insert(4, metrics)\n",
    "    return result\n",
    "\n",
    "# covs = [sigma**2*np.eye(J) for i in range(len(X))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2225383d-3ad0-4c25-a733-140ddd92ea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_param(real_K, real_alph, pred_K, pred_alph):\n",
    "    print(f\"\\nThe real n. of types is: {real_K}\", f\"\\nThe predicted one is: {pred_K[-1]}\\n\")\n",
    "    real_alph = [v for k, v in real_alph.items()]\n",
    "    real_alph = list(np.reshape(real_alph, (-1, real_K)).mean(axis=0))\n",
    "    pred_alph_vis = np.array(pred_alph)[np.argsort(pred_alph)][::-1].tolist()\n",
    "    \n",
    "    print(f\"\\nProportion of real user types (avg. across tableaux): \\n\", real_alph)\n",
    "    print(f\"Proportion of predicted user types (sorted): \\n\", pred_alph_vis, \"\\n\")\n",
    "    \n",
    "def eval_ADM(pred_K, pred_alph, centers, X_test, Y_test, plot_=False, print_=True, real_K=False, real_alph=False):\n",
    "    '''\n",
    "    Function to evaluate the Adaptive Dirac Mixture Algorithm, computing r2 and rmse\n",
    "    pred_K: list of all K predicted\n",
    "    pred_alph: list of all weights predicted\n",
    "    plot_: string (name of file to save 3d-plotted centers to)\n",
    "    '''\n",
    "    if (real_K==True) and (real_alph==True):\n",
    "        comp_param(real_K, real_alph, pred_K, pred_alph)\n",
    "    \n",
    "    preds = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        pt = 0.0\n",
    "        for k in range(pred_K[-1]):\n",
    "            pt += pred_alph[k]*np.exp(-np.dot(centers[k], X_test[i]))\n",
    "        preds.append(1-pt)  # storing predictions\n",
    "    preds = np.array(preds)\n",
    "    r2 = r2_score(Y_test, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(Y_test, preds))\n",
    "    if print_:\n",
    "        print(f\"\\nThe R2 score is: {r2: .5%}\", \"-\"*5, f\"The RMSE is: {rmse}\", \"-\"*5, \"\\n\")\n",
    "    if type(plot_)==str:\n",
    "        plot_centers(centers, plot_)\n",
    "    return preds, r2, rmse\n",
    "\n",
    "def plot_centers(centers_, name_file):\n",
    "    J = len(centers_[0])\n",
    "    centers_ = np.reshape(centers_, (len(centers_), J))\n",
    "    proj_centers = PCA(n_components=3).fit_transform(centers_)\n",
    "    \n",
    "    fig = plt.figure(figsize = (10, 10))\n",
    "    ax = plt.axes(projection =\"3d\")\n",
    " \n",
    "    # Creating plot\n",
    "    ax.scatter3D(proj_centers[:, 0], proj_centers[:, 1], proj_centers[:, 2], s=70, c = np.abs(proj_centers[:, 2]), cmap=\"RdYlGn\")   #  color = \"green\"\n",
    "                          \n",
    "    ax.set_title(\"3D Scatter plot of fitted centers\", fontsize=18)\n",
    "    ax.set_xlabel('PCA-One', fontsize=12)\n",
    "    ax.set_ylabel('PCA-Two', fontsize=12)\n",
    "    ax.set_zlabel('PCA-Three', fontsize=12)\n",
    "    \n",
    "    fig.savefig('images/' + name_file + '.jpg', bbox_inches='tight', dpi=300)   \n",
    "                          \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ab5f8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ADM(K_list, func_eval, thresh, lasso, r2_ls = False, th_list=False): \n",
    "    '''\n",
    "    Function to plot evolution of number of centers considered (K) and associated Loss evaluations, for each iteration considered\n",
    "    K_list: list of all numbers of centers considered between iteration\n",
    "    func_eval: list of Loss evaluations, for each iteration (excluding lasso penalization)\n",
    "    th_list: list of threshold used\n",
    "    r2_ls: list of [r2, rmse] to be plotted\n",
    "    thresh and lasso: only for visualization\n",
    "    '''\n",
    "    \n",
    "    K_list = K_list[1:]  # not considering Init value (1)\n",
    "    assert len(K_list) == len(func_eval), 'to check' # indeed they differ (without above) since K_list is inizialied with one value at Init (it is longer by 1)\n",
    "    iterations = np.array(list(range(1, len(func_eval)+1)))\n",
    "    if type(r2_ls)==list:\n",
    "        r2_ls = np.reshape(r2_ls, (len(K_list), 2))\n",
    "    \n",
    "    var = [K_list, r2_ls] if type(r2_ls)==np.ndarray else [K_list, func_eval]\n",
    "    tit = ['predicted K',  r'$R^2$ and RMSE'] if type(r2_ls)==np.ndarray else ['predicted K', 'Loss evaluations']\n",
    "    lab = ['K values', r'$R^2$ and RMSE'] if type(r2_ls)==np.ndarray else ['K values', 'Mean Squared Error']\n",
    "    \n",
    "    if th_list:\n",
    "        var.append(th_list)\n",
    "        tit.append('thresholds used')\n",
    "        lab.append('Threshold')\n",
    "    \n",
    "    if th_list:\n",
    "        fix, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 8*3))\n",
    "        axs = [ax1, ax2, ax3]\n",
    "    else:\n",
    "        fix, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8*2))\n",
    "        axs = [ax1, ax2]\n",
    "    to_plot = [var, tit, lab]\n",
    "    # plotting and visualizatiom for titles\n",
    "    for idx, ax in enumerate(axs):            \n",
    "        if idx==0:\n",
    "            ax.plot(iterations, to_plot[0][idx], color=\"blue\")\n",
    "            ax.set_ylabel(f\"{to_plot[2][idx]}\", fontsize=14, color=\"blue\")\n",
    "        elif idx==1 and type(r2_ls)!=np.ndarray:\n",
    "            ax.plot(iterations, to_plot[0][idx], color=\"red\")\n",
    "            ax.set_ylabel(f\"{to_plot[2][idx]}\", fontsize=14, color=\"red\")\n",
    "            \n",
    "        elif idx==1 and type(r2_ls)==np.ndarray:\n",
    "            ax2 = ax.twinx()\n",
    "            \n",
    "            ax.plot(iterations, to_plot[0][idx][:, 0], color=\"darkcyan\", label=f\"$R^2$\")  # r2\n",
    "            ax.set_ylim(bottom=-1, top=np.amax(to_plot[0][idx][:, 0]))\n",
    "            ax.set_yticks(ax.get_yticks().tolist())\n",
    "            ax.set_yticklabels([f'{x: .1%}' for x in  ax.get_yticks().tolist()])\n",
    "            ax.legend(loc='upper right', fontsize='large')\n",
    "            ax.set_ylabel(f\"$R^2$\", fontsize=15, color=\"darkcyan\")\n",
    "            \n",
    "                    \n",
    "            ax2.plot(iterations, to_plot[0][idx][:, 1], color=\"crimson\", label=\"RMSE\")  # rmse\n",
    "            ax2.legend(loc='lower right', fontsize='large')\n",
    "            ax2.set_ylabel(\"RMSE\", fontsize=14, color=\"crimson\")\n",
    "        else:\n",
    "            ax.plot(iterations, to_plot[0][idx], color=\"orangered\")\n",
    "            ax.set_ylabel(f\"{to_plot[2][idx]}\", fontsize=14, color=\"orangered\")\n",
    "        \n",
    "        title_for_thresh = \"with adaptive thresh\" if thresh==None else f\"thresh = {thresh}\"  #{thresh: .4f}\n",
    "        title_for_lasso = \"without lasso pen\" if lasso==0 else f\"lasso pen={lasso}\"   #{lasso: .4f}\n",
    "        \n",
    "        if idx==0:\n",
    "            ax.set_title(f\"Evolution of {to_plot[1][idx]}, {title_for_lasso} and {title_for_thresh}\", fontsize=15)\n",
    "        else:\n",
    "            ax.set_title(f\"Evolution of {to_plot[1][idx]}\", fontsize=15)\n",
    "        ax.set_xlabel(\"Number of iterations\", fontsize=14)\n",
    "        \n",
    "    plt.show()\n",
    "    return fix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4343ea6-23e2-47ae-8147-ad202e978c01",
   "metadata": {},
   "source": [
    "### Initial study of simulated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c392e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def time_creation(P, J, K, MoyLCM, Mass0, N_tableaux):\n",
    "    '''\n",
    "    Function to display the time for creation of dataset\n",
    "    '''\n",
    "    start = timer()                # st = time.process_time()\n",
    "    T, Types, theta, alpha_proportion, poids_0 = dataset_LCM(P, J, K, MoyLCM, Mass0, N_tableaux)  # random poids in [0, 1]\n",
    "    timer(start, \"creating the Tableaux\")        # elapsed = time.process_time() - st, print('CPU time:', elapsed, 'seconds')\n",
    "    return T, Types, theta, alpha_proportion, poids_0    \n",
    "\n",
    "def export_gen(P, J, K, MoyLCM, Mass0, N_tableaux, to_store=5):\n",
    "    '''\n",
    "    Fuction to store as an excel file a sample of 5 (default) dataset\n",
    "    it comprises the Theta matrix (parameters of Poisson), in common among all subcampaigns as well as Alpha proportions (varying between sub-campaigns)\n",
    "    '''\n",
    "    FilePath = os.path.realpath(\"Visualize.xlsx\")\n",
    "    #ExcelWorkbook = openpyxl.load_workbook(FilePath)\n",
    "    writer = pd.ExcelWriter(FilePath, engine = 'openpyxl')\n",
    "    T, Types, theta, alpha_proportion, poids_0 = dataset_LCM(P, J, K, MoyLCM, Mass0, N_tableaux)  # random poids in [0, 1]\n",
    "    pad_vert = pd.DataFrame(np.zeros((P, 1)), columns=[\"\"])\n",
    "    pad_vert.replace(0, np.nan, inplace=True)\n",
    "    pad_1 = np.zeros((P-K, J))\n",
    "    pad_1[:] = np.nan\n",
    "    # just stacking arrays for visualization\n",
    "    left = pd.DataFrame(data=np.vstack((theta, pad_1)), columns=[\"theta cookie \" + str(x + 1) for x in range(J)])\n",
    "    \n",
    "    pad_2 = pd.DataFrame(np.zeros((P-1, K)), index=np.arange(1, P))\n",
    "    pad_2[:] = np.nan\n",
    "    \n",
    "    for i in range(to_store):\n",
    "        alpha = alpha_proportion[i].reshape(1, -1)\n",
    "        alpha = pd.DataFrame(data = alpha, columns=['proportion ' + str(x) for x in range(K)])\n",
    "        right = pd.concat([alpha, pad_2], axis=0, ignore_index=True)\n",
    "        right = right.iloc[:, :K]\n",
    "        result = pd.concat([T[i], pad_vert, left, pad_vert, right], axis=1, ignore_index=True)\n",
    "        result.columns = T[i].columns.to_list() + pad_vert.columns.to_list() + left.columns.to_list() + pad_vert.columns.to_list() + alpha.columns.to_list()\n",
    "        result.to_excel(writer, sheet_name='Sheet '+str(i+1), na_rep='', index=False, columns=result.columns.to_list())\n",
    "    writer.save()\n",
    "    writer.close()\n",
    "    return T, Types, theta, alpha_proportion, poids_0\n",
    "\n",
    "def export_data(X, Y):\n",
    "    '''\n",
    "    Function to store in an excel file the data fed to the models (both regressor and dependent)\n",
    "    '''\n",
    "    n_tableau = X.shape[0]\n",
    "    assert X.shape[0]==Y.shape[0], 'incorrect data format'\n",
    "    FilePath = os.path.realpath(\"Dataset.xlsx\")\n",
    "    writer = pd.ExcelWriter(FilePath, engine = 'openpyxl')\n",
    "    \n",
    "    pad_vert = pd.DataFrame(np.zeros((n_tableau, 1)), columns=[\"\"])\n",
    "    pad_vert.replace(0, np.nan, inplace=True)\n",
    "    result = pd.concat([X, pad_vert, Y], axis=1, ignore_index=True)\n",
    "    result.columns = X.columns.to_list() + pad_vert.columns.to_list() + Y.columns.to_list()\n",
    "    result.to_excel(writer, sheet_name='Data', na_rep='', index=False, columns=result.columns.to_list())\n",
    "        \n",
    "    writer.save()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0681f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae5059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736ae44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bbf477c-70b5-4c01-88a1-b85c4ca224b6",
   "metadata": {},
   "source": [
    "### Additional Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee1b6b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K, P = 3, 20\n",
    "# True_Z=np.random.randint(0,K,size=P)\n",
    "# alpha_proportion = {}\n",
    "# alpha_proportion=[0 for i in range(K)]\n",
    "# Z=True_Z\n",
    "# print(Counter(Z))\n",
    "# for i in Counter(Z):   # sorted\n",
    "#    print(i)\n",
    "#    alpha_proportion[i]=Counter(Z)[i]/len(Z)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0878ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T_aux, cs, reach, lsum = result[0]\n",
    "# cs = np.array(cs, dtype=int).reshape(1, J)\n",
    "# reach = np.array(reach, dtype=float).reshape(1, 1)\n",
    "\n",
    "# start = timer()\n",
    "# for i in range(1, len(result)):\n",
    "#     t, c, rec, lsum = result[i]\n",
    "#     cs = np.vstack([cs, np.array(c, dtype=int).reshape(1, J)])\n",
    "#     reach = np.vstack([reach, np.array(rec, dtype=float).reshape(1, 1)])\n",
    "\n",
    "# X = pd.DataFrame(data=cs, columns=['cookie ' + str(i+1) for i in range(J)])\n",
    "# Y = pd.DataFrame(data=reach, columns=['reach estimate'])\n",
    "# timer(start)\n",
    "# print(X.shape)\n",
    "# print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c516d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES FOR THRESHOLD\n",
    "\n",
    "# for threshold\n",
    "\n",
    "#decimal=1\n",
    "#while P/10**decimal>=10:\n",
    "#    decimal+=1\n",
    "#for k in range(K):\n",
    "#        if alpha[k]<10**(-decimal):   # threshold\n",
    "#            alpha[k]=0\n",
    "\n",
    "#decimal=1\n",
    "#while K/decimal>=1:\n",
    "    #decimal+=1\n",
    "#print(f\"threshold: {K*10**(-decimal): .2f}\")\n",
    "#for q in range(len(alpha)):\n",
    "#    if alpha[q]<K*10**(-decimal):   # threshold\n",
    "#        alpha[q]==0\n",
    "\n",
    "\n",
    "#decimal = 0\n",
    "#while K*10/10**decimal>=1:\n",
    "    #decimal+=1\n",
    "#threshold = 10**(-decimal)*(K-1)\n",
    "#print(f\"threshold: {threshold}\")\n",
    "\n",
    "#threshold = float(1/K)\n",
    "\n",
    "#with threshold = float(1/K**2) R2 -0.000008 and RMSE 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194a4faf-b710-4cfa-8964-368523eeb488",
   "metadata": {},
   "source": [
    "### print variance of explained by components selected\n",
    "\n",
    "def snn(Tab, model, n_comp=None):\n",
    "    '''\n",
    "    function implementing Synthethic Nearest Neighbour algorithm for our specific setup\n",
    "    Tab: dict result of preprocessing (with keys=campaigns)\n",
    "    model: sklearn estimator, with fit and predict methods\n",
    "    n_components: if None all components are tried and the evolution of mean squared error is plotted\n",
    "                  if fixed to a number (say n), considered only first n components\n",
    "    '''\n",
    "    r2, rmse = [], []\n",
    "    \n",
    "    P = Tab[0][0].shape[1]-1\n",
    "    n_campaigns = len(Tab)\n",
    "    max_comp = Tab[0][0].shape[0]-1\n",
    "    \n",
    "    if ((n_comp != None) and n_comp>max_comp) or ((n_comp != None) and n_comp<1):\n",
    "        print(f\"The number of components must be between 1 and {max_comp}\")\n",
    "        return\n",
    "    \n",
    "    pca_lst = [PCA(n_components=n_comp)] if n_comp!=None else [PCA(n_components=int(i)) for i in range(1, max_comp+1)]\n",
    "    preds, real = [], []\n",
    "    \n",
    "    for camp, df in Tab.items():\n",
    "        data = pd.DataFrame(df[0].T)\n",
    "        \n",
    "        y = data.iloc[:, 0].values\n",
    "        y_train = y[:-1].reshape(-1, 1)\n",
    "        \n",
    "        real.append(df[1])\n",
    "        \n",
    "        x = data.iloc[:, 1:].values\n",
    "        x_train = x[:-1, :]\n",
    "        x_test = x[-1, :].reshape(1, -1)\n",
    "        \n",
    "        for pca in pca_lst:            \n",
    "            sc = StandardScaler()\n",
    "            x_train_pca = pca.fit_transform(sc.fit_transform(x_train))\n",
    "            x_test_pca = pca.transform(x_test)  # test how x_test is transformed\n",
    "            trained = model.fit(x_train_pca, y_train)\n",
    "            pred = float(trained.predict(x_test_pca)[0])\n",
    "            if len(pca_lst)>1 and pca==pca_lst[0]:\n",
    "                preds.append([pred])\n",
    "            elif len(pca_lst)>1:\n",
    "                preds[-1].append(pred)\n",
    "            else:\n",
    "                preds.append(pred)\n",
    "                \n",
    "    real_values = np.array(real)\n",
    "    pred_values = np.array(preds).reshape(n_campaigns, 1) if n_comp!=None else np.reshape(preds, (n_campaigns, len(pca_lst)))\n",
    "    \n",
    "    if n_comp!=None:\n",
    "        r2 = r2_score(real_values, pred_values)\n",
    "        rmse = np.sqrt(mean_squared_error(real_values, pred_values))\n",
    "        print(f\"\\n SNN based on PCR with {int(n_comp)} component(s)\\n\")\n",
    "        print(f\"R2 score across {n_campaigns} campaigns: {r2: .5%}\")\n",
    "        print(f\"Average RMSE score across {n_campaigns} campaigns: {rmse}\")\n",
    "    else:\n",
    "        lst_r2 = [r2_score(real_values, pred_values[:, i]) for i in range(len(pca_lst))]\n",
    "        lst_rmse = [np.sqrt(mean_squared_error(real_values, pred_values[:, i])) for i in range(len(pca_lst))]\n",
    "        assert np.argmax(lst_r2)==np.argmin(lst_rmse), \"check\"\n",
    "        \n",
    "        print(f\"\\n SNN with analysis on number of components, {n_campaigns} campaigns\")\n",
    "        print(f\"Highest R2 and RMSE score are obtained with {np.argmax(lst_r2)+1} component(s)\")\n",
    "        for i in range(1, max_comp+1):\n",
    "            print(f\"\\n{int(i)} Principal component(s) selected \\n R2: {lst_r2[i-1]: .5%} \\n RMSE: {lst_rmse[i-1]}\\n\")\n",
    "            \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(14, 5))\n",
    "        plt.plot(list(range(1, max_comp+1)), lst_r2)\n",
    "        plt.xlabel('Number of Principal Components')\n",
    "        plt.ylabel(r'$R^2$ score')\n",
    "        plt.title(r\"Evolution of $R^2$ with n. of PC selected\")       \n",
    "        \n",
    "        \n",
    "#x_test_stand = np.divide(x_test-(P*sc.mean_), np.sqrt(P)*sc.scale_) \n",
    "#x_test (vector of sum of cookies) is not normalized before PCA since would become 0, it is assumded to be normalized by P (or not=\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bc0c81-a5ba-4d29-b534-679afd26f4ca",
   "metadata": {},
   "source": [
    "### function for snn data engineering select all P\n",
    "\n",
    "def snn_dataset_sim(Tableau, normalize_sum_reach=False):\n",
    "    '''\n",
    "    (Timed) function that takes as input the simulated dataset (dict with keys=campaign ID)\n",
    "    used to add 1st row filled with {0, 1} and last column with sum of collect cookies (for each j in J since transposed)\n",
    "    \n",
    "    the first value in the last column (position 0) is therefore the reach computed as int, which is stored and \n",
    "    substituted with a NaN value to be predicted by the SNN algorithm\n",
    "    \n",
    "    normalize_sum_reach: bool, whether to normalized last column (c mapped to t, and reach to reach/P)\n",
    "    '''\n",
    "    \n",
    "    P = Tableau[0].shape[0]\n",
    "    J = Tableau[0].shape[1]-1  # there is 'user' column\n",
    "    \n",
    "    # start timer\n",
    "    start = timer()\n",
    "    result = {}\n",
    "    \n",
    "    # computations reach and vector of counts for each sub-campaign\n",
    "    for n in range(len(Tableau)):\n",
    "        T_aux = Tableau[n].to_numpy().T  # transposed\n",
    "        T_aux = discretize_columns(T_aux)  # discretization (first row)\n",
    "        \n",
    "        vector_sum=np.nansum(T_aux, axis=1).reshape((-1, 1))\n",
    "        if normalize_sum_reach:\n",
    "            vector_sum = np.divide(vector_sum, T_aux.shape[1])   # normalized t_i vector and reach \n",
    "        \n",
    "        reach_comp = float(vector_sum[0, 0])\n",
    "        vector_sum[0, 0] = np.nan        \n",
    "        final = np.hstack((T_aux, vector_sum))\n",
    "        \n",
    "        # result stores everything, separately we build regressors (normalized vector of cookies count) and dependent (calculation of reach function)\n",
    "        result[n]=(final, reach_comp)\n",
    "    \n",
    "    # visualization of timer with description of task\n",
    "    timer(start, \"preprocessing the dataset for SNN\")    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0d1634-2724-4a61-b127-53345603b9d7",
   "metadata": {},
   "source": [
    "def snn_dataset_sim(Tableau, normalize_sum_reach=False, P_panelist=100):\n",
    "    '''\n",
    "    (Timed) function that takes as input the simulated dataset (dict with keys=campaign ID)\n",
    "    used to add 1st row filled with {0, 1} and last column with sum of collect cookies (for each j in J since transposed)\n",
    "    \n",
    "    the first value in the last column (position 0) is therefore the reach computed as int, which is stored and \n",
    "    substituted with a NaN value to be predicted by the SNN algorithm\n",
    "    \n",
    "    normalize_sum_reach: bool, whether to normalized last column (c mapped to t, and reach to reach/P)\n",
    "    '''\n",
    "    \n",
    "    P = Tableau[0].shape[0]\n",
    "    P_panelist = P if P<P_panelist else P_panelist\n",
    "    J = Tableau[0].shape[1]-1  # there is 'user' column\n",
    "    \n",
    "    # start timer\n",
    "    start = timer()\n",
    "    result = {}\n",
    "    \n",
    "    # computations reach and vector of counts for each sub-campaign\n",
    "    for n in range(len(Tableau)):\n",
    "        T_aux = Tableau[n].to_numpy().T  # transposed\n",
    "        T_aux = discretize_columns(T_aux[1:, :])  # discretization (first row)\n",
    "        \n",
    "        vector_sum=np.nansum(T_aux, axis=1).reshape((-1, 1))\n",
    "        if normalize_sum_reach:\n",
    "            vector_sum = np.divide(vector_sum, P)   # normalized t_i vector and reach \n",
    "        \n",
    "        reach_comp = float(vector_sum[0, 0])\n",
    "        vector_sum[0, 0] = np.nan   \n",
    "        \n",
    "        slice_columns = np.arange(P, dtype=int)   # not taken care of rappresentation of group K\n",
    "        random_cols = np.random.choice(slice_columns, size=P_panelist, replace=False)\n",
    "        T_panelist = T_aux[:, random_cols]\n",
    "           \n",
    "        final = np.hstack((T_panelist, vector_sum))\n",
    "        \n",
    "        # result stores everything, separately we build regressors (normalized vector of cookies count) and dependent (calculation of reach function)\n",
    "        result[n]=(final, reach_comp)\n",
    "    \n",
    "    # visualization of timer with description of task\n",
    "    timer(start, \"preprocessing the dataset for SNN\")    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2efffac4-140c-44d9-9791-c5de9f4d08af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopping condition\n",
    "\n",
    "#check = pd.Series(data=fun_eval, dtype=float).diff()\n",
    "#print(f\"Considered: {np.abs(check.tail(5).array).reshape(-1, 1).tolist()}\")\n",
    "#if np.any(np.abs(check.tail(5).array).reshape(-1, 1) < 2e-5, axis=1).sum() > 3:   # norm of difference between Loss evaluations of last 3 iterations must be all less than 0.00002\n",
    "    #or np.any(np.abs(check.tail(3).array).reshape(-1, 1) < 2e-3, axis=1).sum() > 2\n",
    "    #if print_:\n",
    "        #print(f'\\n\\nSTOPPING condition (stability of loss) at iter {b}\\n\\n')\n",
    "    #break\n",
    "            \n",
    "#if len(fun_eval) > 4:\n",
    "    #if (fun_eval[-4] < 0.1*float(round(fun_eval[-5], 2))) and (float(round(fun_eval[-5], 2)<1)):\n",
    "        #if print_:\n",
    "            #print(f'\\n\\nSTOPPING condition (big initial drop) at iter {b}\\n\\n')\n",
    "        #break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
